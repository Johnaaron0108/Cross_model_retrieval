
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Prototypes in TensorFlow\n",
    "This notebook implements:\n",
    "- Visual Question Answering (VQA) using CNN + LSTM fusion.\n",
    "- Cross-Modal Retrieval (CLIP-style) with dual encoders.\n",
    "\n",
    "Lightweight, Colab-ready, for undergrad learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Text Data (Questions/Labels)\n",
    "For simplicity, we'll create toy questions like `is this a plane?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "\n",
    "# create simple yes/no questions\n",
    "def make_questions(images, labels):\n",
    "    questions, answers = [], []\n",
    "    for lbl in labels.flatten():\n",
    "        obj = classes[lbl]\n",
    "        q = f\"is this a {obj}?\"\n",
    "        questions.append(q)\n",
    "        answers.append(1)  # always yes (toy)\n",
    "    return questions, np.array(answers)\n",
    "\n",
    "train_q, train_a = make_questions(x_train, y_train)\n",
    "test_q, test_a = make_questions(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer for Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_q)\n",
    "\n",
    "max_len = 6\n",
    "train_seq = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(train_q), maxlen=max_len)\n",
    "test_seq = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(test_q), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: VQA Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_in = layers.Input(shape=(32,32,3))\n",
    "base_cnn = ResNet50(weights=None, include_top=False, input_shape=(32,32,3), pooling=\"avg\")\n",
    "img_feat = base_cnn(img_in)\n",
    "\n",
    "txt_in = layers.Input(shape=(max_len,))\n",
    "emb = layers.Embedding(len(tokenizer.word_index)+1, 16)(txt_in)\n",
    "txt_feat = layers.LSTM(32)(emb)\n",
    "\n",
    "fusion = layers.Concatenate()([img_feat, txt_feat])\n",
    "out = layers.Dense(2, activation=\"softmax\")(fusion)\n",
    "\n",
    "vqa_model = models.Model([img_in, txt_in], out)\n",
    "vqa_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "vqa_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = vqa_model.fit([x_train, train_seq], train_a, epochs=1, batch_size=64,\n",
    "                    validation_data=([x_test, test_seq], test_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Cross-Modal Retrieval Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_encoder():\n",
    "    inp = layers.Input(shape=(32,32,3))\n",
    "    base = ResNet50(weights=None, include_top=False, input_shape=(32,32,3), pooling=\"avg\")\n",
    "    x = base(inp)\n",
    "    x = layers.Dense(128)(x)\n",
    "    return models.Model(inp, x)\n",
    "\n",
    "def build_text_encoder():\n",
    "    inp = layers.Input(shape=(max_len,))\n",
    "    x = layers.Embedding(len(tokenizer.word_index)+1, 32)(inp)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    return models.Model(inp, x)\n",
    "\n",
    "img_enc = build_image_encoder()\n",
    "txt_enc = build_text_encoder()\n",
    "\n",
    "# simple forward pass example\n",
    "img_emb = img_enc(x_train[:10])\n",
    "txt_emb = txt_enc(train_seq[:10])\n",
    "print(\"Embeddings shape:\", img_emb.shape, txt_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss (InfoNCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(img_emb, txt_emb, temperature=0.07):\n",
    "    img_emb = tf.math.l2_normalize(img_emb, axis=-1)\n",
    "    txt_emb = tf.math.l2_normalize(txt_emb, axis=-1)\n",
    "    logits = tf.matmul(img_emb, txt_emb, transpose_b=True) / temperature\n",
    "    labels = tf.range(tf.shape(logits)[0])\n",
    "    loss_i2t = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    loss_t2i = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "    return tf.reduce_mean(loss_i2t + loss_t2i)\n",
    "\n",
    "# demo loss\n",
    "print(\"Loss demo:\", contrastive_loss(img_emb, txt_emb).numpy())"
   ]
  }
 ]
}
