# Cross-Modal Retrieval ğŸ”€

<p align="center">
  <img src="https://img.shields.io/badge/Language-Python-3776AB?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Framework-PyTorch-EE4C2C?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Library-SentenceTransformers-6A0DAD?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Library-NumPy-013243?style=for-the-badge"/>
</p>

---

## ğŸ“Œ Overview
A deep learning project that maps **images and text** into a **shared embedding space** for retrieval tasks.  
Enables **image-to-text** and **text-to-image** searches using **contrastive learning**.

---

## ğŸ› ï¸ Features
- Image embeddings using **ResNet50**  
- Text embeddings with **Sentence Transformers**  
- Trained using **contrastive InfoNCE loss**  
- Supports multi-modal retrieval: image-to-text and text-to-image  
- Scalable to large datasets using embeddings

---

## ğŸ“¦ Installation / Setup
1. **Clone the repository**
```bash
git clone https://github.com/Johnaaron0108/Cross_model_retrieval.git
cd Cross_model_retrieval
